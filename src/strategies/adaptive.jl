"""
    AdaptiveParticleSwarm(n_particles = 3,
                          w = 0.9,
                          c1 = 2.0,
                          c2 = 2.0,
                          prob_shift = 0.25,
                          rng = Random.GLOBAL_RNG)

Instantiate an adaptive particle swarm optimization tuning strategy. A swarm is
initiated by sampling hyperparameters with their customizable priors, and new
models are generated by referencing each member's and the swarm's best models so
far.

### Supported ranges

A single one-dimensional range or vector of one-dimensional ranges can be
specified. `ParamRange` objects are constructed using the `range` method. If not
paired with a prior, then one is fitted and truncated if bounded, as follows:

| Range Types             | Default Distribution                        |
|:----------------------- |:------------------------------------------- |
| `NominalRange`          | `Dirichlet([1, 1, ..., 1])`                 |
| Bounded `NumericRange`  | `Uniform(lower, upper)`                     |
| Positive `NumericRange` | `Gamma(α=(origin/unit)^2, θ=unit^2/origin`) |
| Other `NumericRange`    | `Normal(origin, unit)`                      |

Specifically, in `AdaptiveParticleSwarm`, the `range` field of a `TunedModel`
instance can be:

- a single one-dimensional range (`ParamRange` object) `r`

- a pair of the form `(r, d)`, with `r` as above and where `d` is:

    - a Dirichlet distribution with the same number of categories as `r.values`
    (for `NominalRange` `r`)

    - any `Distributions.UnivariateDistribution` *instance* (for `NumericRange`
    `r`)

    - one of the distribution *types* in the table below, for automatic fitting
    using `Distributions.fit(d, r)` to a distribution whose support always
    lies between `r.lower` and `r.upper` (for `NumericRange` `r`)

- any vector of objects of the above form

| Range Types             | Distribution Types                                                                           |
|:----------------------- |:-------------------------------------------------------------------------------------------- |
| Bounded `NumericRange`  | `Arcsine`, `Uniform`, `Biweight`, `Cosine`, `Epanechnikov`, `SymTriangularDist`, `Triweight` |
| Positive `NumericRange` | `Gamma`, `InverseGaussian`, `Poisson`                                                        |
| Any `NumericRange`      | `Normal`, `Logistic`, `LogNormal`, `Cauchy`, `Gumbel`, `Laplace`                             |

### Examples

using Distributions

range1 = range(model, :hyper1, lower=0, upper=1)

range2 = [(range(model, :hyper1, lower=1, upper=10), Arcsine),
range(model, :hyper2, lower=2, upper=Inf, unit=1, origin=3),
(range(model, :hyper2, lower=2, upper=4), Normal(0, 3)),
range(model, :hyper3, values=[:ball, :tree])]

### Algorithm

Hyperparameter ranges are sampled and concatenated into position vectors for
each swarm particle. Velocity is initiated to be zeros, and in each iteration,
every particle's position is updated to approach its personal best and the
swarm's best models so far with the equations:

\$vₖ₊₁ = w⋅vₖ + c₁⋅rand()⋅(pbest - xₖ) + c₂⋅rand()⋅(gbest - xₖ)\$

\$xₖ₊₁ = xₖ + vₖ₊₁\$

Coefficients `w`, `c1`, `c2` are adaptively adjusted at each iteration by
determining the evolutionary phase of the swarm. We calculate the evolutionary
factor by comparing the mean distance from each particle to other members of the
swarm. This factor is then used to classify whether the swarm is in exploration,
exploitation, convergence, or jumping out phase and calibrate the tuning
hyperparameters accordingly. For more information, refer to "Adaptive Particle
Swarm Optimiztion" by Zhan, Zhang, Li, and Chung. Note that we omit the elitist
learning strategy in the paper.

New models are then generated for evaluation by mutating the fields of a deep
copy of `model`. If the corresponding range has a specified `scale` function,
then the transformation is applied before the hyperparameter is returned. If
`scale` is a symbol (eg, `:log`), it is ignored.

### Discrete Hyperparameter Handling

Since particle swarm is an optimization method for continuous problems, integer
and nominal hyperparameters require special handling: they are converted to
continuous values, and transformed back to their original domains at each step
for evaluation.

For integer `NumericRange`s, a continuous distribution is fitted to generate
initial values for the swarm. They are then rounded when each particle is mapped
to the corresponding candidate model.

`NominalRange`s on the other hand are represented as categorical distributions
over their values. Hence, we use Dirichlet prior distributions to initialize a
probability vector for each particle, defaulting to the uniform distribution
Dirichlet([1, 1, ..., 1]). The same velocity and position updates apply, but
probability values are further clamped in the range [0, 1] and normalized to sum
up to 1. When a better model is found, we replicate both its probability vector
and sampled value by shifting unchosen categories' weights towards the selected
one for pbest and gbest models:

\$pᵢ = (1 - prob_shift) * pᵢ\$

\$pₛ = pₛ + prob_shift\$

where pₛ is the probability of the sampled hyperparameter value. For more
information, refer to "A New Discrete Particle Swarm Optimization Algorithm" by
Strasser, Goodman, Sheppard, and Butcher.
"""
mutable struct AdaptiveParticleSwarm{R<:AbstractRNG} <: AbstractParticleSwarm
    n_particles::Int
    w::Float64
    c1::Float64
    c2::Float64
    prob_shift::Float64
    rng::R
    # TODO: topology
end

# Constructor

function AdaptiveParticleSwarm(;
    n_particles=3,
    w=0.9,
    c1=2.0,
    c2=2.0,
    prob_shift=0.25,
    rng::R=Random.GLOBAL_RNG
) where {R}
    swarm = AdaptiveParticleSwarm{R}(n_particles, w, c1, c2, prob_shift, rng)
    message = MLJTuning.clean!(swarm)
    isempty(message) || @warn message
    return swarm
end

# Validate tuning hyperparameters

function MLJTuning.clean!(tuning::AdaptiveParticleSwarm)
    warning = ""
    if tuning.n_particles < 3
        warning *= "AdaptiveParticleSwarm requires at least 3 particles. " *
                   "Resetting n_particles=3. "
        tuning.n_particles = 3
    end
    c1, c2 = tuning.c1, tuning.c2
    if !(1.5 ≤ c1 ≤ 2.5) || !(1.5 ≤ c2 ≤ 2.5) || (c1 + c2 > 4)
        _clamp_coefficients!(tuning)
        warning *= "AdaptiveParticleSwarm requires 1.5 ≤ c1 ≤ 2.5, 1.5 ≤ c2 ≤ 2.5, and " *
                   "c1 + c2 ≤ 4. Resetting coefficients c1=$(tuning.c1), c2=$(tuning.c2). "
    end
    if !(0 ≤ tuning.prob_shift < 1)
        warning *= "AdaptiveParticleSwarm requires 0 ≤ prob_shift < 1. " *
                   "Resetting prob_shift=0.25. "
        tuning.prob_shift = 0.25
    end
    return warning
end

# Helper function to clamp swarm coefficients in the interval [1.5, 2.5] with a sum of less
# than or equal to 4

function _clamp_coefficients!(tuning)
    c1 = tuning.c1 = min(max(tuning.c1, 1.5), 2.5)
    c2 = tuning.c2 = min(max(tuning.c1, 1.5), 2.5)
    scale = 4. / (c1 + c2)
    if scale < 1
        tuning.c1 = c1 * scale
        tuning.c2 = c2 * scale
    end
    return tuning
end

# Initial state

function MLJTuning.setup(tuning::AdaptiveParticleSwarm, model, ranges, n, verbosity)
    # state and evolutionary phase
    return (initialize(ranges, tuning), nothing)
end

# New models

function MLJTuning.models(
    tuning::AdaptiveParticleSwarm,
    model,
    history,
    (state, phase),
    n_remaining,
    verbosity
)
    n_particles = tuning.n_particles
    if !isnothing(history)
        sig = MLJTuning.signature(first(history).measure)
        measurements = similar(state.pbest)
        map(history[end-n_particles+1:end]) do h
            measurements[h.metadata] = sig * h.measurement[1]
        end
        pbest!(state, tuning, measurements)
        gbest!(state, tuning)
        f, phase = _evolutionary_state(tuning, state, phase)
        _adapt_parameters!(tuning, state, f, phase)
        move!(state, tuning)
    end
    retrieve!(state, tuning)
    fields = getproperty.(state.ranges, :field)
    new_models = map(1:n_particles) do i
        clone = deepcopy(model)
        for (field, param) in zip(fields, getindex.(state.parameters, i))
            recursive_setproperty!(clone, field, param)
        end
        (clone, i)
    end
    return new_models, (state, phase)
end

# Helper function to calculate the evolutionary factor and phase

function _evolutionary_state(tuning, state, phase)
    n_particles = tuning.n_particles
    X = state.X
    dists = zeros(n_particles, n_particles)
    for i in 1:n_particles
        for j in i+1:n_particles
            dists[j, i] = dists[i, j] = norm(X[i, :] - X[j, :])
        end
    end
    mean_dists = sum(dists, dims=2) / (n_particles - 1)
    min_dist, max_dist = extrema(mean_dists)
    gbest_dist = mean_dists[argmin(state.pbest)]
    f = (gbest_dist - min_dist) / max(max_dist - min_dist, sqrt(eps()))

    # Classify evolutionary phase
    μs = zeros(4)
    μs[1] = μ₁(f)
    μs[2] = μ₂(f)
    μs[3] = μ₃(f)
    μs[4] = μ₄(f)
    if phase === nothing # first iteration
        phase = argmax(μs)
    else
        next_phase = mod1(phase + 1, 4)
        # switch to next phase if possible
        if μs[next_phase] > 0
            phase = next_phase
        # stay in current phase is possible, else pick the most likely phase
        elseif μs[phase] == 0
            phase = argmax(μs)
        end
    end
    return f, phase
end

# Helper functions to calculate probabilities of the four evolutionary states

μ₁(f) = f ≤ 0.4 ? 0.0         :
        f ≤ 0.6 ? 5 * f - 2   :
        f ≤ 0.7 ? 1           :
        f ≤ 0.8 ? -10 * f + 8 :
        0.0

μ₂(f) = f ≤ 0.2 ? 0.0        :
        f ≤ 0.3 ? 10 * f - 2 :
        f ≤ 0.4 ? 1.0        :
        f ≤ 0.6 ? -5 * f + 3 :
        0.0

μ₃(f) = f ≤ 0.1 ? 1.0          :
        f ≤ 0.3 ? -5 * f + 1.5 :
        0.0

μ₄(f) = f ≤ 0.7 ? 0.0         :
        f ≤ 0.9 ? 5 * f - 3.5 :
        1.0

# Adaptive control of swarm's parameters

function _adapt_parameters!(tuning, state, f, phase)
    tuning.w = 1.0 / (1.0 + 1.5*exp(-2.6 * f)) # update inertia
    δ = rand(tuning.rng) * 0.05 + 0.05 # coefficient acceleration
    if phase === 1 # exploration
        tuning.c1 += δ
        tuning.c2 -= δ
    elseif phase === 2 # exploitation
        δ *= 0.5
        tuning.c1 += δ
        tuning.c2 -= δ
    elseif phase === 3 # convergence
        δ *= 0.5
        tuning.c1 += δ
        tuning.c2 += δ
    else # jumping out
        tuning.c1 -= δ
        tuning.c2 += δ
    end
    _clamp_coefficients!(tuning)
    return tuning
end