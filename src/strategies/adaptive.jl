"""
    AdaptiveParticleSwarm(n_particles = 3,
                          c1 = 2.0,
                          c2 = 2.0,
                          prob_shift = 0.25,
                          rng = Random.GLOBAL_RNG)

Instantiate an adaptive particle swarm optimization tuning strategy. A swarm is
initiated by sampling hyperparameters with their customizable priors, and new
models are generated by referencing each member's and the swarm's best models so
far.

### Supported Ranges and Discrete Hyperparameter Handling

See [`ParticleSwarm`](@ref) for more information about supported ranges and how
discrete hyperparameters are handled.

### Algorithm

Hyperparameter ranges are sampled and concatenated into position vectors for
each swarm particle. Velocity is initiated to be zeros, and in each iteration,
every particle's position is updated to approach its personal best and the
swarm's best models so far with the equations:

\$vₖ₊₁ = w⋅vₖ + c₁⋅rand()⋅(pbest - xₖ) + c₂⋅rand()⋅(gbest - xₖ)\$

\$xₖ₊₁ = xₖ + vₖ₊₁\$

Coefficients `w`, `c1`, `c2` are adaptively adjusted at each iteration by
determining the evolutionary phase of the swarm. We calculate the evolutionary
factor by comparing the mean distance from each particle to other members of the
swarm. This factor is then used to classify whether the swarm is in exploration,
exploitation, convergence, or jumping out phase and calibrate the tuning
hyperparameters accordingly. For more information, refer to "Adaptive Particle
Swarm Optimiztion" by Zhan, Zhang, Li, and Chung. Note that we omit the elitist
learning strategy in the paper.

New models are then generated for evaluation by mutating the fields of a deep
copy of `model`. If the corresponding range has a specified `scale` function,
then the transformation is applied before the hyperparameter is returned. If
`scale` is a symbol (eg, `:log`), it is ignored.
"""
mutable struct AdaptiveParticleSwarm{R<:AbstractRNG} <: AbstractParticleSwarm
    n_particles::Int
    c1::Float64
    c2::Float64
    prob_shift::Float64
    rng::R
end

# Constructor

function AdaptiveParticleSwarm(;
    n_particles=3,
    c1=2.0,
    c2=2.0,
    prob_shift=0.25,
    rng::R=Random.GLOBAL_RNG
) where {R}
    swarm = AdaptiveParticleSwarm{R}(n_particles, c1, c2, prob_shift, rng)
    message = MLJTuning.clean!(swarm)
    isempty(message) || @warn message
    return swarm
end

# Validate tuning hyperparameters

function MLJTuning.clean!(tuning::AdaptiveParticleSwarm)
    warning = ""
    if tuning.n_particles < 3
        warning *= "AdaptiveParticleSwarm requires at least 3 particles. " *
                   "Resetting n_particles=3. "
        tuning.n_particles = 3
    end
    c1, c2 = tuning.c1, tuning.c2
    if !(1.5 ≤ c1 ≤ 2.5) || !(1.5 ≤ c2 ≤ 2.5) || (c1 + c2 > 4)
        c1, c2 = _clamp_coefficients(c1, c2)
        warning *= "AdaptiveParticleSwarm requires 1.5 ≤ c1 ≤ 2.5, 1.5 ≤ c2 ≤ 2.5, and " *
                   "c1 + c2 ≤ 4. Resetting coefficients c1=$(c1), c2=$(c2). "
        tuning.c1 = c1
        tuning.c2 = c2
    end
    if !(0 ≤ tuning.prob_shift < 1)
        warning *= "AdaptiveParticleSwarm requires 0 ≤ prob_shift < 1. " *
                   "Resetting prob_shift=0.25. "
        tuning.prob_shift = 0.25
    end
    return warning
end

# Helper function to clamp swarm coefficients in the interval [1.5, 2.5] with a sum of less
# than or equal to 4

function _clamp_coefficients(c1, c2)
    c1 = min(max(c1, 1.5), 2.5)
    c2 = min(max(c2, 1.5), 2.5)
    scale = 4. / (c1 + c2)
    if scale < 1
        c1 *= scale
        c2 *= scale
    end
    return c1, c2
end

# Initial state

function MLJTuning.setup(tuning::AdaptiveParticleSwarm, model, ranges, n, verbosity)
    # state, evolutionary phase, swarm coefficients
    return (initialize(ranges, tuning), nothing, tuning.c1, tuning.c2)
end

# New models

function MLJTuning.models(
    tuning::AdaptiveParticleSwarm,
    model,
    history,
    (state, phase, c1, c2),
    n_remaining,
    verbosity
)
    n_particles = tuning.n_particles
    if !isnothing(history)
        sig = MLJTuning.signature(history[1].measure[1])
        measurements = similar(state.pbest)
        map(history[end-n_particles+1:end]) do h
            measurements[h.metadata] = sig * h.measurement[1]
        end
        pbest!(state, measurements, tuning)
        gbest!(state)
        f = _evolutionary_factor(state.X, argmin(state.pbest))
        phase = _evolutionary_phase(f, phase)
        w, c1, c2 = _adapt_parameters(tuning.rng, c1, c2, f, phase)
        move!(tuning.rng, state, w, c1, c2)
    end
    retrieve!(state, tuning)
    fields = getproperty.(state.ranges, :field)
    new_models = map(1:n_particles) do i
        clone = deepcopy(model)
        for (field, param) in zip(fields, getindex.(state.parameters, i))
            recursive_setproperty!(clone, field, param)
        end
        (clone, i)
    end
    return new_models, (state, phase, c1, c2)
end

# Helper function to calculate the evolutionary factor and phase

function _evolutionary_factor(X, gbest_i)
    n_particles = size(X, 1)
    dists = zeros(n_particles, n_particles)
    for i in 1:n_particles
        for j in i+1:n_particles
            dists[j, i] = dists[i, j] = norm(X[i, :] - X[j, :])
        end
    end
    mean_dists = sum(dists, dims=2) / (n_particles - 1)
    min_dist, max_dist = extrema(mean_dists)
    gbest_dist = mean_dists[gbest_i]
    f = (gbest_dist - min_dist) / max(max_dist - min_dist, sqrt(eps()))
    return f
end

function _evolutionary_phase(f, phase)
    # Classify evolutionary phase
    μs = [μ₁(f), μ₂(f), μ₃(f), μ₄(f)]
    if phase === nothing # first iteration
        phase = argmax(μs)
    else
        next_phase = mod1(phase + 1, 4)
        # switch to next phase if possible
        if μs[next_phase] > 0
            phase = next_phase
        # stay in current phase is possible, else pick the most likely phase
        elseif μs[phase] == 0
            phase = argmax(μs)
        end
    end
    return phase
end

# Helper functions to calculate probabilities of the four evolutionary states

μ₁(f) = f ≤ 0.4 ? 0.0         :
        f ≤ 0.6 ? 5 * f - 2   :
        f ≤ 0.7 ? 1           :
        f ≤ 0.8 ? -10 * f + 8 :
        0.0

μ₂(f) = f ≤ 0.2 ? 0.0        :
        f ≤ 0.3 ? 10 * f - 2 :
        f ≤ 0.4 ? 1.0        :
        f ≤ 0.6 ? -5 * f + 3 :
        0.0

μ₃(f) = f ≤ 0.1 ? 1.0          :
        f ≤ 0.3 ? -5 * f + 1.5 :
        0.0

μ₄(f) = f ≤ 0.7 ? 0.0         :
        f ≤ 0.9 ? 5 * f - 3.5 :
        1.0

# Adaptive control of swarm's parameters

function _adapt_parameters(rng, c1, c2, f, phase)
    w = 1.0 / (1.0 + 1.5*exp(-2.6 * f)) # update inertia
    δ = rand(rng) * 0.05 + 0.05 # coefficient acceleration
    if phase === 1 # exploration
        c1 += δ
        c2 -= δ
    elseif phase === 2 # exploitation
        δ *= 0.5
        c1 += δ
        c2 -= δ
    elseif phase === 3 # convergence
        δ *= 0.5
        c1 += δ
        c2 += δ
    else # jumping out
        c1 -= δ
        c2 += δ
    end
    c1, c2 = _clamp_coefficients(c1, c2)
    return w, c1, c2
end

function MLJTuning.tuning_report(
    tuning::AdaptiveParticleSwarm,
    history,
    (state, phase, c1, c2)
)
    fields = getproperty.(state.ranges, :field)
    scales = MLJBase.scale.(state.ranges)
    return (; plotting = MLJTuning.plotting_report(fields, scales, history))
end